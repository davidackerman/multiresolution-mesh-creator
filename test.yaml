   
##
## When using flyemflows, name this file 'dask-config.yaml',
## and place it in your template directory (next to workflow.yaml).
##
## To use this config in a different project (not using flyemflows),
## then activate these config settings via an environment variable:
##
##  export DASK_CONFIG=/path/to/example-dask-config.yaml
##

# In addition to the explanations below, it may be useful to see the default config
# settings used by dask, distributed, and dask-jobqueue, which can be found here:
#
# - https://github.com/dask/dask/blob/master/dask/dask.yaml
# - https://github.com/dask/distributed/blob/master/distributed/distributed.yaml
# - https://github.com/dask/dask-jobqueue/blob/master/dask_jobqueue/jobqueue.yaml
#

##
## General 'distributed' scheduler settings
##

# I tend to use the default settings, so most of this section is empty.
distributed:
  admin:
    # I like to customize the log format (used by Python's logging module).
    # (flyemflows sets it this way for you.)
    log-format: '[%(asctime)s] %(levelname)s %(message)s'

  scheduler:
    work-stealing: False

  worker:
    # dask has various strategies for dealing with a worker that is consuming too much RAM,
    # including "spilling" data to disk, "pausing" the worker until its RAM usage has declined,
    # or terminating the worker and restarting it as a last resort.
    # If you think you know what you're doing, you can set any of these values to 0.0 to
    # disable that strategy entirely.
    #
    # Note: If you need to inspect your RAM usage, flyemflows writes a file named
    #       'graph-links.txt' which provides handy access to the LSF RTM graphs for your workers.
    memory:
      #target: 0.0
      #spill: 0.0
      #pause: 0.0
      terminate: 0.0

##
## HPC cluster settings (i.e. 'dask-jobqueue' settings)
##

# These settings determine how each *clump* of workers is allocated, as described below.
#
# Explanation:
# 
# Dask coordinates your computation using one of its "schedulers".
# Dask itself comes with three built-in schedulers, which run on a single machine.
# These are 'single-threaded', 'threads', or 'processes'.
#
# But for running on a compute cluster, there is a fourth scheduler,
# implemented in a separate package, named 'distributed'.
#
# To use the 'distributed' scheduler, you must first start up dask worker processes on
# each of the nodes you want to use, and point them back to the master scheduler process.
# If you wanted, you could do this by ssh'ing to each worker node and launching
# each dask-worker from the command line.
#
# But as a convenience, the dask project also provides a little package that knows how to
# launch those dask-worker processes for you, via LSF's 'bsub' commmand.  It's named 'dask-jobqueue'.
# Hence, the config settings above apply to workloads that use the 'distributed' scheduler in general
# (regardless of how the cluster was launched), while the settings below apply to LSF-specific
# settings, and determine how the workers are launched and configured using bsub.
#
# dask-jobqueue launches dask workers in clumps (one bsub call launches one clump).
# The clump will consist of N processes (i.e. N workers), sharing M threads (M cores) in total.
#
# Note: In flyemflows, launchflow -n ${N} will give you at least ${N} worker *processes*,
#       regardless of how many threads they'll use, or how many 'clumps' that requires.
#       Furthermore, if your value of ${N} is not a perfect multiple of the clump size,
#       then you'll be given a few more worker processes than you needed.
#       For example, if your clump size is 31 and you ask for N=100 workers, then it will
#       launch four clumps, for a total of 4*31 = 124 workers.


jobqueue:
  lsf:
    # Total worker processes in each clump.
    processes: 31

    # Total threads in each clump.
    # Typically, we want each worker to use only a single thread,
    # so we set cores == processes.
    # If you were to set cores = 2*processes, then each worker
    # process will use a threadpool (of 2 threads).
    cores: 31

    # Optionally, we can ask LSF to *reserve* more slots than the
    # number of threads we'll actually use for the computation.
    #
    # If our workload will be RAM-intensive, then it's often
    # useful to reserve more slots than we'll be using for dask workers,
    # for two reasons:
    #  (1) If we need a lot of extra RAM, we might want to make ncpus == 2*cores
    #  (2) Even if we don't need much extra RAM, it's a good idea to leave one core
    #      free for garbage collection, to avoid spurious RAM spikes if the
    #      garbage collector thread is temporarily starved.
    #
    # If we omit this setting, it defaults to ncpus == cores.
    ncpus: 32
    
    # How much memory should dask permit this clump of workers to use?
    # At Janelia, each slot is permitted to use up to 15 GB of RAM.
    # If a worker approaches (or exceeds) this limit, there are various ways
    # the distributed scheduler might try to bring the RAM usage back down.
    # (See the 'memory' section above, for example.)
    memory: 480GB # 15GB * 32 slots
    
    # This is used by the LSF reservation system.
    # By default, it is equivalent to the 'memory' setting above, but specified in bytes.
    # (To be honest, I'm not sure if our LSF cluster even looks at this setting.)
    mem: 480000000000
    
    # This specifies how long these jobs will last before LSF kills them (bsub -W)
    # If your computation will be short, set this to 1 hour (or less)
    # to make it eligible for Janelia's "short queue"
    # In general, shorter times get higher priority on the cluster,
    # but if your job exceeds the limit you write here, it will be killed by LSF.
    walltime: '01:00'
    
    # Implementation detail regarding how bsub is called by dask-jobqueue.
    # Under Janelia's LSF configuration, this must be set to 'true'.
    use-stdin: true

    # Where to dump worker logs (not the main client log).
    # Basically workers will be launched like this:
    #
    #   bsub -o {log-directory}/worker-NNNN.log ...
    #
    # Careful: If you leave this blank, LSF will send you a lot of emails!
    log-directory: dask-worker-logs
    
    # If dask needs to store temporary data for caching or whatever, it will use this directory.
    # On the Janelia cluster, the /scratch/<user> directory is the best place for temporary data.
    # In flyemflows, this is automatically set for you (if you leave it blank).
    # 
    #local-directory: /scratch/<INSERT YOUR USERNAME HERE>

    # The name of the worker jobs, i.e. what you'll see when you check 'bjobs'
    name: dask-worker
    
    # Extra arguments to bsub can be provided here.
    # For example, this charges the job to flyem's budget, rather than your default account.
    job-extra: ["-P flyem"]


# If you prefer to launch your dask-workers in clumps of just 1 core each,
# you can use the following settings.  But this is less ideal than the
# above setup, for three reasons:
#
#   (1) See the explanation of 'ncpus' regarding RAM and garbage collection.
#   (2) If you launch your workers one-at-a-time, then 'bjobs' is less pleasant to interpret.
#   (3) Janelia's LSF RTM UI produces one page of graphs per clump, so you can view all of
#       your worker statistics with fewer clicks if you don't create a separate clump for
#       every worker.
#
#jobqueue:
#  lsf:
#    processes: 1
#    cores: 1
#    memory: 15GB
#    walltime: '01:00'
#    log-directory: dask-worker-logs
#    local-directory: /scratch/<INSERT YOUR USERNAME HERE>
